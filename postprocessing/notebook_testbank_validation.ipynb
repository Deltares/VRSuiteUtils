{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35352a3f",
   "metadata": {},
   "source": [
    "This notebook can be used to quickly generate additional output for comparing results. It is based on the test cases run on TeamCity as part of the continuous testing framework for the VRTOOL. However, it can also be used to directly compare 2 different databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746917ce-1090-4f5f-83c2-c5af70a80faa",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b85c72-0b27-4530-97c4-3b1443ac1b81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from peewee import fn\n",
    "\n",
    "from vrtool.orm.models import *\n",
    "from vrtool.orm.orm_controllers import open_database\n",
    "from vrtool.common.enums import MechanismEnum\n",
    "from postprocessing.database_analytics import *\n",
    "from postprocessing.database_access_functions import * \n",
    "from postprocessing.generate_output import *\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"colorblind\", 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4b5e2-745d-4adc-9539-5d4c224ecf32",
   "metadata": {},
   "source": [
    "### Get the runs that are in both databases\n",
    "First we get an overview of the runs in both databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a768e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = Path(r'c:\\Users\\klerk_wj\\OneDrive - Stichting Deltares\\00_Projecten\\11_VR_HWBP\\01_voor_waterschappen\\ZZL\\test_v0.1+\\run_2\\traject_7_2_nieuw.db')\n",
    "result_runs = get_overview_of_runs(result_path)\n",
    "\n",
    "reference_path = Path(r'n:\\Projects\\11209000\\11209353\\B. Measurements and calculations\\008 - Resultaten Proefvlucht\\ZZL\\7-2\\timings Edwin\\bugfix_Edwin\\traject_7_2.db')\n",
    "reference_runs = get_overview_of_runs(reference_path)\n",
    "\n",
    "pd.DataFrame(result_runs + reference_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19f47",
   "metadata": {},
   "source": [
    "Now we set the ids of the runs in result & reference. \n",
    "Additionally, we set whether we want to consider revetment or not (important later on) and whether we want to take the last investment step, or the optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_for_result = 1\n",
    "id_for_reference = 1\n",
    "has_revetment = True    #Whether a revetment is present or not\n",
    "take_last = False       #If True, the last step is taken, if False, the step with minimal total cost is taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a608a1da",
   "metadata": {},
   "source": [
    "For each run, we get the optimization steps and the index of the step with minimal total costs. This is the optimal combination of measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917433c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_steps = {'reference': get_optimization_steps_for_run_id(reference_path, id_for_reference),\n",
    "                      'result': get_optimization_steps_for_run_id(result_path, id_for_result)}\t\n",
    "\n",
    "# add total cost as sum of total_lcc and total_risk in each step\n",
    "if not take_last:\n",
    "    considered_tc_step = {'reference': get_minimal_tc_step(optimization_steps['reference'])-1,\n",
    "                        'result': get_minimal_tc_step(optimization_steps['result'])-1}\n",
    "else:\n",
    "    considered_tc_step = {'reference': len(optimization_steps['reference'])-1,\n",
    "                        'result': len(optimization_steps['result'])-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ec572",
   "metadata": {},
   "source": [
    "This plot shows the results for total cost and risk for both runs. \n",
    "If these values are equal it means that the steps are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "markers = ['d', 'o']\n",
    "for count, run in enumerate(optimization_steps.keys()):\n",
    "    plot_lcc_tc_from_steps(optimization_steps[run], axis=ax, lbl  = run, clr = colors[count])\n",
    "    ax.plot(optimization_steps[run][considered_tc_step[run]]['total_lcc'], optimization_steps[run][considered_tc_step[run]]['total_risk'], markers[count], color = colors[count])\n",
    "ax.set_xlabel('Total LCC')\n",
    "ax.set_ylabel('Total risk')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(top=1e10)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2141620",
   "metadata": {},
   "source": [
    "This figure gives us insight in the difference between the reference and the new result: in principle the points generated for the new result should be closer to the origin than that of the reference, otherwise the performance of the optimization has worsened. However, also large changes should of course be explained. \n",
    "\n",
    "The latter is the focus of the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb8c43",
   "metadata": {},
   "source": [
    "### Reading measures per step\n",
    "The next step is to read the measures and parameters of these measures for each optimization step such that we can compare the measures that are taken in each step and for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ee97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_of_measures = {'reference': get_measures_for_run_id(reference_path, id_for_reference),\n",
    "                     'result': get_measures_for_run_id(result_path, id_for_result)}\n",
    "\n",
    "measures_per_step = {'reference': get_measures_per_step_number(lists_of_measures['reference']),\n",
    "                        'result': get_measures_per_step_number(lists_of_measures['result'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43463ead",
   "metadata": {},
   "source": [
    "If we want to see the failure probability per stap we first need to load the original assessment for each mechanism, and then we can compute the reliability for each step during the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2963f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_results = {'reference': {}, 'result': {}}\n",
    "for mechanism in [MechanismEnum.OVERFLOW, MechanismEnum.PIPING, MechanismEnum.STABILITY_INNER, MechanismEnum.REVETMENT]:\n",
    "    if has_revetment or mechanism != MechanismEnum.REVETMENT:\n",
    "        assessment_results['reference'][mechanism] = import_original_assessment(reference_path, mechanism)\n",
    "        assessment_results['result'][mechanism] = import_original_assessment(result_path, mechanism)\n",
    "\n",
    "\n",
    "reliability_per_step = {'reference': get_reliability_for_each_step(reference_path, measures_per_step['reference']),\n",
    "                        'result': get_reliability_for_each_step(result_path, measures_per_step['result'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d7f82",
   "metadata": {},
   "source": [
    "We need to check if the assessment results are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO add comparison of assessment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6134da",
   "metadata": {},
   "source": [
    "Based on these inputs we can make a stepwise_assessment based on the investments in reliability_per_step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936221a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_assessment = {'reference': assessment_for_each_step(copy.deepcopy(assessment_results['reference']), reliability_per_step['reference']),\n",
    "                        'result': assessment_for_each_step(copy.deepcopy(assessment_results['result']), reliability_per_step['result'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28078569",
   "metadata": {},
   "source": [
    "The next step is to derive the traject probability for each mechanism for each step using the `calculate_traject_probability_for_steps` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traject_prob = {'reference': calculate_traject_probability_for_steps(stepwise_assessment['reference']),\n",
    "                'result': calculate_traject_probability_for_steps(stepwise_assessment['result'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b427617",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots()\n",
    "\n",
    "plot_traject_probability_for_step(traject_prob['reference'][0], ax, run_label='Beginsituatie referentie',color=colors[0], linestyle = '--')\n",
    "plot_traject_probability_for_step(traject_prob['result'][0], ax, run_label='Beginsituatie resultaat',color=colors[1], linestyle=':')\n",
    "plot_traject_probability_for_step(traject_prob['reference'][considered_tc_step['reference']], ax, run_label='Referentie',color=colors[0], linestyle = '-')\n",
    "plot_traject_probability_for_step(traject_prob['result'][considered_tc_step['result']], ax, run_label='Resultaat', color=colors[1], linestyle='-')\n",
    "ax.set_xlim(left=0,right=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628e349",
   "metadata": {},
   "source": [
    "Now we are going to print the MeasureResultIds for each section for both runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_per_section = {'reference': get_measures_per_section_for_step(measures_per_step['reference'], considered_tc_step['reference']+1),\n",
    "                        'result': get_measures_per_section_for_step(measures_per_step['result'], considered_tc_step['result']+1)}\n",
    "\n",
    "for section in set(list(measures_per_section['result'].keys())+ list(measures_per_section['reference'].keys())):\n",
    "    for run in measures_per_section.keys():\n",
    "        try:\n",
    "            print(f\"Section {section} in run {run} has measures {measures_per_section[run][section]}\")  \n",
    "        except:\n",
    "            print(f\"Section {section} in run {run} has no measures in run {run}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca08be4",
   "metadata": {},
   "source": [
    "Based on this we can see if the ids of the measures are identical. If these are identical & the measures given as input are also identical this shows that the result is the same. \n",
    "\n",
    "However, to further check this we also get the parameters of the measure + timing + cost for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_parameters = {'reference':{}, 'result': {}}\n",
    "\n",
    "for section in measures_per_section['reference'].keys():\n",
    "    section_parameters['reference'][section] = []\n",
    "    for measure in measures_per_section['reference'][section][0]:\n",
    "        parameters = get_measure_parameters(measure, reference_path)\n",
    "        parameters.update(get_measure_costs(measure, reference_path))\n",
    "        parameters.update(get_measure_type(measure, reference_path))\n",
    "        section_parameters['reference'][section].append(parameters)\n",
    "\n",
    "for section in measures_per_section['result'].keys():\n",
    "    section_parameters['result'][section] = []\n",
    "    for measure in measures_per_section['result'][section][0]:\n",
    "        parameters = get_measure_parameters(measure, result_path)\n",
    "        parameters.update(get_measure_costs(measure, result_path))\n",
    "        parameters.update(get_measure_type(measure, result_path))\n",
    "        section_parameters['result'][section].append(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b2883",
   "metadata": {},
   "source": [
    "Next we put this in a DataFrame such that we can easily compare the DataFrames of both runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142cac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_parameters = {'reference': measure_per_section_to_df(measures_per_section['reference'], section_parameters['reference']),\n",
    "                        'result': measure_per_section_to_df(measures_per_section['result'], section_parameters['result'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2889b6",
   "metadata": {},
   "source": [
    "Potentially we can export this to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084df249",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_parameters['reference'].to_csv('reference_measures.csv')\n",
    "measure_parameters['result'].to_csv('result_measures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349539d5",
   "metadata": {},
   "source": [
    "## Analysis of reliability index\n",
    "Finally we can take a look into the reliability indices that have been computed in both runs. \n",
    "\n",
    "For instance, if a certain section has a relatively high $\\beta$ this might indicate that the measure at that section is suboptimal and could give clues for inaccuracies of the algorithm. \n",
    "\n",
    "First we pick `t_analyzed`, the time at which we analyze the $\\beta$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: this variable sets the time at which the beta is analyzed\n",
    "t_analyzed = 0\n",
    "\n",
    "#get betas for each section and mechanism at t_analyzed\n",
    "betas_per_section_and_mech = {'result': get_beta_for_each_section_and_mech_at_t(stepwise_assessment['result'][considered_tc_step['result']], t_analyzed),\n",
    "                                'reference': get_beta_for_each_section_and_mech_at_t(stepwise_assessment['reference'][considered_tc_step['reference']], t_analyzed)}\n",
    "\n",
    "# transform dicts to dataframe\n",
    "for run in betas_per_section_and_mech.keys():\n",
    "    betas_per_section_and_mech[run] = pd.DataFrame.from_dict(betas_per_section_and_mech[run]).rename_axis('section_id').reset_index()\n",
    "    betas_per_section_and_mech[run]['run'] = run \n",
    "\n",
    "betas_per_section_and_mechanism = pd.concat([betas_per_section_and_mech['result'], betas_per_section_and_mech['reference']], ignore_index = True)\n",
    "betas_per_section_and_mechanism = pd.melt(betas_per_section_and_mechanism, id_vars = ['section_id', 'run'], var_name = 'mechanism', value_name = 'beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a085a",
   "metadata": {},
   "source": [
    "Next we make a plot to compare the $\\beta$ for both runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison_of_beta_values(betas_per_section_and_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a869d",
   "metadata": {},
   "source": [
    "Differences can also be revealing. First we plot the betas for different mechanisms on a diagonal axis to identify if any values deviate and by how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_difference_in_betas(betas_per_section_and_mechanism, has_revetment)\n",
    "#TODO add automatic labels of SectionId or something similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33090fc",
   "metadata": {},
   "source": [
    "Next we do the same, but subtract the values and display per section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_difference_in_betas_per_section(betas_per_section_and_mechanism, has_revetment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
